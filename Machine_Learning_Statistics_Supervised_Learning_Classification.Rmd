---
title: "Machine Learning with R"
author: "Luiz Felipe de Almeida Brito"
date: "03/06/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Supervised Learning  - Machine Learning with R
### Machine learning algorithms covered in this script:
 - Support Vector Machines
 - Decision Tree
 - Random Forest
 - Support Vector Machines + PCA

### What is the main objective? What am I trying to predict?
The main purpose of this experiment is predicted if there is the presence or not of heart disease in the patient. 

### Data Set Information:
This database contains 76 attributes, but I am using a subset of 14 of them. 
The "goal" field refers to the presence of heart disease in the patient. 
It is integer valued from 0 (no presence) to 4. 
Heart Disease UCI: https://www.kaggle.com/ronitf/heart-disease-uci

### 1º Step - Clear Workspace
```{r, echo = TRUE}
rm(list = ls())   
```

### 2º Step - Clear console
```{r, echo = TRUE}
cat("\014")      
```

### 3º Step - The packages below must be installed. Once installed, you can comment this chunk code.
- e1071: Support Vector Machine (SVM)
- tree: Decision Tree
- randomForest: Random Forest
- caret: Classification and Regression Training
- dplyr: A Grammar of Data Manipulation
- ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics

### 4º Step - Load libraries.
```{r, echo = TRUE}
library(e1071)    
library(tree)     
library(randomForest)
library(caret)       
library(dplyr)       
library(ggplot2)
```

### 5º Step - Set up my work directory.
```{r, echo = TRUE}
setwd("D:\\Machine_Learning")
```

### 6º Step - Reading my database.
```{r, echo = TRUE}
database <- read.csv("heart.csv", header = TRUE)
```

### 7º Step - Only 14 attributes are used. Renaming columns in order to make our exploratory analysis easier.
- 1  (age) 				Age in years 
- 2  (sex) 				Sex (1 = male; 0 = female)
- 3  (cp) 				  Chest pain type -- Value 0: typical angina -- Value 1: atypical angina -- Value 2: non-anginal pain -- Value 3: asymptomatic
- 4  (trestbps) 		Resting blood pressure (in mm Hg on admission to the hospital)
- 5  (chol) 				Serum cholestoral in mg/dl
- 6  (fbs) 				Fasting blood sugar > 120 mg/dl (1 = true; 0 = false)
- 7  (restecg) 		Resting electrocardiographic results -- Value 0: normal / Value 1: having ST-T wave abnormality / Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria 
- 8  (thalach) 		 Maximum heart rate achieved
- 9  (exang) 			 Exercise induced angina (1 = yes; 0 = no) 
- 10 (oldpeak) 		 ST depression induced by exercise relative to rest
- 11 (slope) 			 The slope of the peak exercise ST segment -- Value 0: upsloping / Vaue 1: flat / Value 2: downsloping
- 12 (ca) 				   Number of major vessels (0-3) colored by flourosopy 
- 13 (thal) 				 3 = normal; 6 = fixed defect; 7 = reversable defect
- 14 (target)       The predicted attribute

```{r, echo = TRUE}
names(database)[names(database) == "ï..age"]   <- "age"
names(database)[names(database) == "sex"]      <- "sex"
names(database)[names(database) == "cp"]       <- "chest_pain_type"
names(database)[names(database) == "trestbps"] <- "resting_blood_pressure"
names(database)[names(database) == "chol"]     <- "serum_cholestoral"
names(database)[names(database) == "fbs"]      <- "fasting_blood_sugar"
names(database)[names(database) == "restecg"]  <- "resting_cardiographic_results"
names(database)[names(database) == "thalach"]  <- "maximum_heart_rate"
names(database)[names(database) == "exang"]    <- "exercise_induced_angina"
names(database)[names(database) == "oldpeak"]  <- "depression_induced_exercise_rest"
names(database)[names(database) == "slope"]    <- "slope_peak_exercise"
names(database)[names(database) == "ca"]       <- "number_major_vessels"
names(database)[names(database) == "thal"]     <- "thal"
names(database)[names(database) == "target"]   <- "target"
```

### 8º Step - Missing Values
Missing Values plays an important role in statistics and data analysis.
Often, missing values must not be ignored, but rather they should be carefully studied to see if there is an underlying pattern or cause for their missingness.
```{r, echo = TRUE}
ncol(database)
```

```{r, echo = TRUE}
nrow(database)
```

```{r, echo = TRUE}
table(is.na.data.frame(database))
```

### 9º Step - After a careful reading of the dataset information it is possible 
###           to infer that the columns below can be transformed into factor types.
###           Factors are used to represent caterorical data. One can think of a ,
###           factor as an integer vector where each integer has a label. 
Using factors with labels is better than using integers because factors are self-describing. Have a variable that has values "Male" and "Female" is better than 1 and 2.
```{r, echo = TRUE}
data_heart        <- database
data_heart$target <- as.factor(data_heart$target)

database$sex                           <- factor(database$sex, levels=c(0, 1), labels=c("female", "male"), ordered = TRUE)
database$chest_pain_type               <- factor(database$chest_pain_type, levels=c(0, 1, 2, 3), labels=c("typical angina", "atypical angina", "non-anginal pain", "asymptomatic"), ordered = TRUE)
database$fasting_blood_sugar           <- factor(database$fasting_blood_sugar, levels=c(0, 1), labels=c(FALSE, TRUE), ordered = TRUE)
database$resting_cardiographic_results <- factor(database$resting_cardiographic_results, levels=c(0, 1, 2), labels=c("normal", "having wave abnormality", "probable ventricular hypertrophy"), ordered = TRUE )
database$exercise_induced_angina       <- factor(database$exercise_induced_angina, levels=c(0, 1), labels=c("No", "Yes"), ordered = TRUE)
database$slope_peak_exercise           <- factor(database$slope_peak_exercise, levels=c(0, 1, 2), labels=c("upsloping", "flat", "downsloping"), ordered = TRUE)
database$number_major_vessels          <- as.numeric(database$number_major_vessels)
database$thal                          <- as.numeric(database$thal)

database$target                        <- factor(database$target, levels=c(0, 1), labels=c("absence", "presence"), ordered = TRUE)
```

STR
```{r, echo = TRUE}
str(database)
```

Summary
```{r, echo = TRUE}
summary(database)
```

### 10º Step - Outlier detection and treatment
Outliers in data can distort predictions and affect the accuracy, if you don't detect and handle them appropriately especially in regression models. Why outliers treatment is important? Because, it can drastically bias/change the fit estimates and predictions. A handy explanation of Outiliers it can be found in : https://www.r-bloggers.com/outlier-detection-and-treatment-with-r/

### 10.1º Step - Detect Outliers - Univariate approach
For a given continuous variable, outliers are those observations that lie outside 1.5 * IQR, where IQR, the 'Inter Quartile Range' is the difference between 75th and 25th quartiles. Look at the points outside the whiskers in below box plot.

Age
```{r age, echo = TRUE}
summary(database$age)
outlier_values <- boxplot.stats(database$age)$out          # outlier values.
boxplot(database$age, main="Detect Outliers - age", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)
```

Sex
```{r sex, echo = TRUE}
summary(database$sex)
outlier_values <- boxplot.stats(database$sex)$out          # outlier values.
boxplot(database$sex, main="Detect Outliers - sex", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)
```

Chest pain type
```{r chest_pain_type, echo = TRUE}
summary(database$chest_pain_type)
outlier_values <- boxplot.stats(database$chest_pain_type)$out          # outlier values.
boxplot(database$chest_pain_type, main="Detect Outliers - chest_pain_type", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)
```

Resting blood pressure
```{r resting_blood_pressure, echo = TRUE}
summary(database$resting_blood_pressure)			 
outlier_values <- boxplot.stats(database$resting_blood_pressure)$out          # outlier values.
boxplot(database$resting_blood_pressure, main="Detect Outliers - resting_blood_pressure", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)
```

Serum cholestoral
```{r serum_cholestoral, echo = TRUE}
summary(database$serum_cholestoral)
outlier_values <- boxplot.stats(database$serum_cholestoral)$out          # outlier values.
boxplot(database$serum_cholestoral, main="Detect Outliers - serum_cholestoral", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)
```

Fasting blood sugar
```{r fasting_blood_sugar, echo = TRUE}
summary(database$fasting_blood_sugar)
outlier_values <- boxplot.stats(database$fasting_blood_sugar)$out          # outlier values.
boxplot(database$fasting_blood_sugar, main="Detect Outliers - fasting_blood_sugar", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)
```

Resting cardiographic results
```{r resting_cardiographic_results, echo = TRUE}
summary(database$resting_cardiographic_results)
outlier_values <- boxplot.stats(database$resting_cardiographic_results)$out          # outlier values.
boxplot(database$resting_cardiographic_results, main="Detect Outliers - resting_cardiographic_results", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)
```

Maximum heart rate
```{r maximum_heart_rate, echo = TRUE}
summary(database$maximum_heart_rate)
outlier_values <- boxplot.stats(database$maximum_heart_rate)$out          # outlier values.
boxplot(database$maximum_heart_rate, main="Detect Outliers - maximum_heart_rate", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)
```

Exercise induced angina
```{r exercise_induced_angina, echo = TRUE}
summary(database$exercise_induced_angina)
outlier_values <- boxplot.stats(database$exercise_induced_angina)$out          # outlier values.
boxplot(database$exercise_induced_angina, main="Detect Outliers - exercise_induced_angina", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)
```

Depression induced exercise rest
```{r depression_induced_exercise_rest, echo = TRUE}
summary(database$depression_induced_exercise_rest)
outlier_values <- boxplot.stats(database$depression_induced_exercise_rest)$out          # outlier values.
boxplot(database$depression_induced_exercise_rest, main="Detect Outliers - depression_induced_exercise_rest", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)
```

Slope peak exercise
```{r slope_peak_exercise, echo = TRUE}
summary(database$slope_peak_exercise)
outlier_values <- boxplot.stats(database$slope_peak_exercise)$out          # outlier values.
boxplot(database$slope_peak_exercise, main="Detect Outliers - slope_peak_exercise", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)
```

Number major vessels
```{r number_major_vessels, echo = TRUE}
summary(database$number_major_vessels)
outlier_values <- boxplot.stats(database$number_major_vessels)$out          # outlier values.
boxplot(database$number_major_vessels, main="Detect Outliers - number_major_vessels", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)
```

thal
```{r thal, echo = TRUE}
summary(database$thal)
outlier_values <- boxplot.stats(database$thal)$out          # outlier values.
boxplot(database$thal, main="Detect Outliers - thal", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)
```

Target
```{r target, echo = TRUE}
summary(database$target)
outlier_values <- boxplot.stats(database$target)$out          # outlier values.
boxplot(database$target, main="Detect Outliers - target", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)
```

### 10.2º Step - We found Outliers only for following continuous variable:
Let's to try a bivariate approach, so that we can visualize in box-plot of the X and Y, for categorical X' (target).

Resting blood pressure
```{r , echo = TRUE}
boxplot(resting_blood_pressure ~ target, data=database, main="resting_blood_pressure (continuos var) vs target")
```

Serum cholestoral
```{r , echo = TRUE}
boxplot(serum_cholestoral ~ target, data=database, main="serum_cholestoral (continuos var) vs target")
```

Maximum heart Rate
```{r , echo = TRUE}
boxplot(maximum_heart_rate ~ target, data=database, main="maximum_heart_rate (continuos var) vs target")
```
Depression induced exercise rest
```{r , echo = TRUE}
boxplot(depression_induced_exercise_rest ~ target, data=database, main="depression_induced_exercise_rest (continuos var) vs target")
```

### 11º Step - Identification Of Near Zero Variance Predictors.
It diagnoses predictors that have one unique value (i.e. are zero variance predictors) or predictors that are have both of the following characteristics: they have very few unique values relative to the number of samples and the ratio of the frequency of the most common value to the frequency of the second most common value is large
```{r , echo = TRUE}
table(nearZeroVar(data_heart))
```

### 12º Step - Exploratory Data Analysis.
Statistical Methods for Exploratory Analysis. Theses methods include clustering and dimension reduction techiques.
It comes to visualization a high dimensional or multidimensional data.
Clustering organizes things that are close into groups.
Probably, the most kind of familiar distance metric is the Euclidean distance which is just kind of the stright-line distance between any two points.
```{r , echo = TRUE}
hc <- hclust(dist(data_heart, method = "euclidean"), method="complete")
plot(hc)
```

Hierarchical Clustering give us an ideia of the relationship between variables or observation.
```{r , echo = TRUE}
plot(as.dendrogram(hc))   
```

### 13º Step - Analytic Graphics.
They help us find patterns in data and understand its properties.
They suggest modeling strategies to communicate results.
                - Show Comparisons.
                - Show causality, mechanism, explanation, systematic structure.
                - Shom multivariate data.
                - Integrate multiple models of evidence.
```{r , echo = TRUE}
png('plot_machine_learning.png')
```

```{r , echo = TRUE}
par(mfrow=c(2,2))   
```

```{r , echo = TRUE}
plot(x = database$target, y = database$age, col = "black", type = "l", xlab = "Target", ylab = "Age", plot = TRUE)
```

```{r , echo = TRUE}
plot(x = database$target, y = database$sex, col = "black", type = "l",  xlab = "Target", ylab = "Sex", plot = TRUE)
```

```{r , echo = TRUE}
plot(x = database$target , y = database$resting_blood_pressure, col = "black", type = "l", xlab = "", ylab = "Outliers", plot = TRUE)
lines(x = database$target, y = database$serum_cholestoral, col = "red")
lines(x = database$target, y = database$maximum_heart_rate, col = "blue")
legend("topright", lty = "solid", col = c("black", "red", "blue"), legend = c("resting_blood_pressure", "serum_cholestoral", "maximum_heart_rate"), bty = "n")
```

```{r , echo = TRUE}
df <- select(data_heart, target, depression_induced_exercise_rest)
ggplot(df, aes(x=depression_induced_exercise_rest)) + geom_histogram(binwidth=1)
```

```{r , echo = FALSE}
par(mfrow=c(1,1))        # sets the plot window back to normal
dev.off()                # But this will clear your plot history.
```

### 14º Step - Setting Random number seed with Set.seed ensure reproducibility.
Always set the random number seed when conducting a simulation.
```{r , echo = TRUE}
set.seed(1)
```

### 15º Step - The training set is used to fit the models;  
The test set is used for assessment of the generalization error of the final chosen model. Ideally, the test set should be kept in a "vault," and be brought out only at the end of the data analysis. It generates randomly the indices for test base. 
We split out 30% for testing and 70% for training.
```{r , echo = TRUE}
indexes = sample(1:nrow(data_heart), size=0.3*nrow(data_heart))
```

```{r , echo = TRUE}
train = data_heart[-indexes,]
test = data_heart[indexes,]
```

### 16º Step - SVM - Support Vector Machines
 - cost or C => Cost of the violations of the restrictions. (constant of Lagrange's regularization)
 - Gamma => It defines the distance of influence of the patterns in the limits of decision (low values => far, high values => near)
```{r , echo = TRUE}
system.time(svm_model <- svm(target ~., train, probability =T))
predictionsSVM <- predict(svm_model, test, probability =T)
table(predictionsSVM,test$target)
acuracy = 1 - mean(predictionsSVM != test$target)
acuracy
summary(svm_model)
```

```{r , echo = TRUE}
probabilities = attr(predictionsSVM, "probabilities")
predictionsAndProbabilities = cbind(test$target, predictionsSVM, probabilities)
View(predictionsAndProbabilities)
```

Metric that evaluates the level of agreement of a classification task
```{r , echo = TRUE}
cm = table(predictionsSVM,test$target); cm
kappa = confusionMatrix(cm)$overall[2]; kappa
```

Confusion MatriX
```{r , echo = TRUE}
confusionMatrix(cm)
```

### 17º Step - Decision Tree 
```{r , echo = TRUE}
system.time(tree_model <- tree(target ~., train))
predictionsDtree <- predict(tree_model, test, type = "class")
table(predictionsDtree, test$target)
acuracy = 1 - mean(predictionsDtree != test$target)
acuracy
summary(tree_model)
```

```{r , echo = TRUE}
plot(tree_model)
```

Metric that evaluates the level of agreement of a classification task
```{r , echo = TRUE}
cm = table(predictionsDtree,test$target); cm
kappa = confusionMatrix(cm)$overall[2]; kappa
```

Confusion MatriX
```{r , echo = TRUE}
confusionMatrix(cm)
```

### 18º Step - Random Forest
```{r , echo = TRUE}
system.time(forest_model <- randomForest(target ~., data = train, 
                                         importance = TRUE, 
                                         do.trace = 100))
predictionsForest = predict(forest_model, test)
table(predictionsForest, test$target)
acuracy = 1 - mean(predictionsForest != test$target)
acuracy
plot(forest_model)
legend("topright", legend=c("OOB", "0", "1"),
       col=c("black", "red", "green"), lty=1:1, cex=0.8)
```

Two measures of importance to rank attributes.
```{r , echo = TRUE}
varImpPlot(forest_model)                 
```

Metric that evaluates the level of agreement of a classification task
```{r , echo = TRUE}
cm = table(predictionsForest,test$target); cm
kappa = confusionMatrix(cm)$overall[2]; kappa
```

Confusion MatriX
```{r , echo = TRUE}
confusionMatrix(cm)
```

### 19º Step - Principal Component Analysis (PCA)
The principal components are equal to the right singular values if you first scale(Subtract the mean, divide by the standard deviation) the variables.
We do not use the "target" column when pca is applied.
```{r , echo = TRUE}
train_pca <- train[-14]         
train_pca <- scale(train_pca)
pca <- princomp(train_pca)
summary(pca)        
```

Pca's results (standard deviation, proportional variance and proportion of cumulative variance)
```{r , echo = TRUE}
plot(pca)
```

### 19.1º Step - Principal Component Analysis (PCA)
We are going to use only The principal components that maintain a cumulative variance of 100% of the total.
```{r , echo = TRUE}
vars <- pca$sdev^2
vars <- vars/sum(vars)
cumulativeVariance <- cumsum(vars)
```

Variance 0.95. Only the first 11 columns.
```{r , echo = TRUE}
View(as.data.frame(cumulativeVariance)) 
```

```{r , echo = TRUE}
train_pca <- pca$scores[,1:11]
train_pca = as.data.frame(train_pca)
train_pca = cbind(train_pca, train$target)
colnames(train_pca)[ncol(train_pca)] <- "target"
```

### 20º Step - SVM - Support Vector Machines + PCA
```{r , echo = TRUE}
system.time(svm_model_pca <- svm(target ~., train_pca, probability =T))
predictionsSVM_PCA <- predict(svm_model_pca, train_pca, probability =T)
table(predictionsSVM_PCA,train_pca$target)
acuracy = 1 - mean(predictionsSVM_PCA != train_pca$target)
acuracy
summary(svm_model_pca)
```

```{r , echo = TRUE}
probabilities = attr(predictionsSVM_PCA, "probabilities")
predictionsAndProbabilities = cbind(test$target, predictionsSVM_PCA, probabilities)
View(predictionsAndProbabilities)
```

Metric that evaluates the level of agreement of a classification task
```{r , echo = TRUE}
cm = table(predictionsSVM_PCA,train_pca$target); cm         
kappa = confusionMatrix(cm)$overall[2]; kappa
```

Confusion MatriX
```{r , echo = TRUE}
confusionMatrix(cm)
```

